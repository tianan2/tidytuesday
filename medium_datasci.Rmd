---
title: "Untitled"
author: "Jaehwan Lim"
date: "January 15, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(stringr)

medium_datasci <- read_csv("https://github.com/rfordatascience/tidytuesday/blob/master/data/2018/2018-12-04/medium_datasci.csv?raw=true") %>% 
  select(-x1)

theme_set(theme_light())
```

```{r}
medium_datasci %>% 
  summarize_at(vars(starts_with("tag_")), sum)

medium_gathered <- medium_datasci %>% 
  ## another way to code the third arguement - starts_with("tag_")
  gather(tag, frequency, c(tag_ai:tag_machine_learning)) %>% 
  ## novel approach to clean the string - str_remove
  mutate(tag = str_remove(tag, "tag_")) %>% 
  ## another step that I haven't figured out
  filter(frequency == 1)
  
medium_gathered %>% 
  count(tag, sort = TRUE)

medium_gathered %>% 
  group_by(tag) %>% 
  summarize(median_claps = median(claps)) %>% 
  arrange(desc(median_claps))

## why do we use median instead of mean?
medium_datasci %>% 
  ggplot(aes(claps)) + 
  geom_histogram() + 
  scale_x_log10()

medium_datasci %>% 
  mutate(reading_time = pmin(10, reading_time)) %>% 
  ggplot(aes(reading_time)) + 
  geom_histogram() + 
  scale_x_continuous(breaks = seq(2, 10, 2), 
                     labels = c(seq(2, 8, 2), "10+"))


```
## Text Mining

```{r}
library(tidytext)

medium_words <- medium_datasci %>% 
  filter(!is.na(title)) %>% 
  transmute(post_id = row_number(), title, subtitle, year, reading_time, claps) %>% 
  unnest_tokens(word, title) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(str_detect(word, "[a-z]")) %>% 
  filter(word != "de") 

medium_words %>% 
  count(word, sort = T) %>% 
  mutate(word = fct_reorder(word, n)) %>%
  head(20) %>% 
  ggplot(aes(word, n)) +
  geom_col() + 
  coord_flip() + 
  labs(title = "Common words in Medium post titles")

```

```{r}
medium_words_filtered <- medium_words %>% 
  add_count(word) %>% 
  filter(n >= 250)

tag_claps <- medium_words_filtered %>% 
  group_by(word) %>% 
  summarize(median_claps = median(claps), 
            # takes the log of claps to compress the scale
            geometric_mean_claps = exp(mean(log(claps + 1))) -1, 
            occurrence = n()) %>%
  arrange(desc(median_claps)) %>% 
  ungroup()

## pakackages
library(widyr)
library(ggraph)
library(igraph)

top_word_cors <- medium_words_filtered %>% 
  select(post_id, word) %>%
  ## what kind of words tend to appear as a pair?
  pairwise_cor(word, post_id, sort = T) %>% 
  head(150)

set.seed(2018)

top_word_cors %>%
  graph_from_data_frame() %>% 
  ggraph() + 
  geom_edge_link() + 
  geom_node_point() +
  geom_node_text(aes(label = name), repel = T) + 
  theme_void()

  
  
```


